---
layout: posts
title: "허밍으로 원곡과 비교해 유사도 알아내기"
date: 2023-03-25
categories:
  - Project
tags: ["SingIt", "Project"]
---

SingIt 을 포기할 수 없어 또 한달간 next.js 와 typescript 를 공부하며 틈틈히 구글링 해보았다.

<br>

전 블로깅에 답한 것처럼 머신 러닝을 개발해 학습시키고 노래를 분석 시키게 하는 방법은 오픈소스가 있지 않은 이상 개인이 학습해 개발하기엔 불가능하였다.

<br>

나는 hum to search 가 아닌 -> hum to compare 로 원곡과 유저가 허밍한 두 곡을 비교하기로 노선을 변경하였고

비교하기 위해 음성 인식의 관련 기술인 MFCC(Mel-Frequency Cepstral Coefficients) 를 추출하여 비교해 보기로 하였다.

<br>

MFCC 는 음성 신호를 분석하여 주파수 성분의 특징을 추출하는 방법으로 여러 라이브러리를 통해서 비교적 쉽게 추출이 가능하다.

<br>

그 중 나는 meyda 라는 라이브러리를 사용해 음성 오디오의 MFCC 를 추출하고 자 진행하였고 ~~실패하였다~~

<br>

> npm install meyda

로 node.js 를 통해 설치가 가능

요즘 유행하는 Chat GPT 의 힘을 빌려 나름 코드를 짜 보았다.

```js
import React, { useState } from "react";
import * as meyda from "meyda";

const socket = new WebSocket("ws://localhost:3000");

socket.onerror = function (event) {
  console.error("WebSocket error:", event);
};

socket.onopen = function (event) {
  console.log("WebSocket connection established");
};

socket.onclose = function (event) {
  console.log("WebSocket connection closed:", event.code, event.reason);
};

socket.onmessage = function (event) {
  console.log("Received message:", event.data);
};

function Acoustic() {
  const audioContext = new AudioContext();
  const [result, setResult] = useState("");
  const [mfcc1, setMFCC1] = useState(null);
  const [mfcc2, setMFCC2] = useState(null);

  // const bufferLength = audioBuffer.length;
  // const sampleRate = audioBuffer.sampleRate;
  const frameSize = 2048;
  const hopSize = 512;
  const fftSize = 2048;
  const melBands = 26;
  const mfccs = 13;

  const featureExtractors = {
    mfcc: {
      extractor: "mfcc",
      bufferSize: frameSize,
      hopSize: hopSize,
      fftSize: fftSize,
      melBands: melBands,
      mfccs: mfccs,
    },
  };

  function euclideanDistance(x, y) {
    let distance = 0;
    for (let i = 0; i < x.length; i++) {
      distance += Math.pow(x[i] - y[i], 2);
    }
    return Math.sqrt(distance);
  }

  function calculateMFCCDistance(mfcc1, mfcc2) {
    let distance = 0;
    for (let i = 0; i < mfcc1.length; i++) {
      distance += euclideanDistance(mfcc1[i], mfcc2[i]);
    }
    return distance / mfcc1.length;
  }

  function calculateDTWDistance(seq1, seq2, distanceFunction) {
    const DTW = [];
    const n = seq1.length;
    const m = seq2.length;
    for (let i = 0; i < n + 1; i++) {
      DTW[i] = [];
      for (let j = 0; j < m + 1; j++) {
        DTW[i][j] = Infinity;
      }
    }
    DTW[0][0] = 0;

    for (let i = 1; i < n + 1; i++) {
      for (let j = 1; j < m + 1; j++) {
        const cost = distanceFunction(seq1[i - 1], seq2[j - 1]);
        DTW[i][j] =
          cost + Math.min(DTW[i - 1][j], DTW[i][j - 1], DTW[i - 1][j - 1]);
      }
    }
    return DTW[n][m];
  }

  const handleButtonClick = () => {
    let audioBuffer1, audioBuffer2;
    let source1, source2;

    fetch("music/example.wav")
      .then((response) => {
        return response.arrayBuffer();
      })
      .then((arrayBuffer) => {
        return audioContext.decodeAudioData(arrayBuffer);
      })
      .then(async (audioBuffer) => {
        audioBuffer1 = audioBuffer;
        const response = await fetch("music/example.mp3");
        const arrayBuffer = await response.arrayBuffer();
        return await audioContext.decodeAudioData(arrayBuffer);
      })
      .then((audioBuffer) => {
        audioBuffer2 = audioBuffer;

        source1 = audioContext.createBufferSource();
        source1.buffer = audioBuffer1;
        source1.connect(audioContext.destination);

        source2 = audioContext.createBufferSource();
        source2.buffer = audioBuffer2;
        source2.connect(audioContext.destination);

        const extractor1 = meyda.createMeydaAnalyzer({
          audioContext: audioContext,
          source: source1,
          featureExtractors: featureExtractors,
          bufferSize: frameSize,
          hopSize: hopSize,
          windowingFunction: "hanning",
        });

        const extractor2 = meyda.createMeydaAnalyzer({
          audioContext: audioContext,
          source: source2,
          featureExtractors: featureExtractors,
          bufferSize: frameSize,
          hopSize: hopSize,
          windowingFunction: "hanning",
        });

        extractor1.get("mfcc", (features) => {
          console.log("mfcc1: ", features);
          setMFCC1(features);
        });

        extractor2.get("mfcc", (features) => {
          console.log("mfcc2: ", features);
          setMFCC2(features);
        });

        extractor1.start();

        source1.start(0);

        source1.onended = () => {
          extractor2.start();
          source2.start(0);
          const calculateDistance = () => {
            if (mfcc1 && mfcc2) {
              console.log(mfcc1, mfcc2);
              setResult(
                calculateDTWDistance(mfcc1, mfcc2, calculateMFCCDistance)
              );
            } else {
              setTimeout(calculateDistance, 100);
            }
          };
          calculateDistance();
        };
      })
      .catch((err) => console.log(err));
  };

  return (
    <>
      <button onClick={handleButtonClick}>Start AudioContext</button>
      <div>정확도는 "{result}" 입니다.</div>
    </>
  );
}

export { Acoustic };
```

<br>
<br>

MFCCs 추출 후, 퍼센테이지로 유사도를 출력하게 짜 보았다.

결론은 실패. r.reduce is not a function 이라는 계속 나오는 오류에 몆 주를 노력해 고쳐봐도 제자리 걸음이였다.

<br>

구글링을 해보아도 관련 오류를 찾을 수 없어서 라이브러리 관련 오류라고 판단.. GPT 는 크롬이 업데이트 되면서 막힌 것이라 다운그레이드를 추천하였고 다운그레이드를 진행해도 web audio api 등 다른 곳에서 디펜던시 오류와 싸워야 했다.

<br>

![image.png](/assets/img/image.png)
~~제발 되게 해주세요~~

<br>

meyda 라이브러리의 문제라면... 내가 직접 Web Audio Api 를 이용해 MFCCs 를 추출하고 비교하는 코드를 짜면 되지 않나?!

<br>

바로 실행에 옮겼다.

이번에도 GPT 의 도움을 받았다. 지금 생각해보면 GPT 때문이라고 탓을 하고 싶지만.. 이번에도 FFT 라이브러리의 문제? 로 실패로 돌아갔다.

MFCCs 을 추출하기 앞서 오디오의 바이너리 데이터를 알고자 ArrayBuffer 객체를 오디오를 통해 만들고 FFT 라이브러리를 통해 MFCCs 계산하는 함수를 짰고 모든 코드를 이해하기 위해 검색하기도 전에 오류로 이틀만에 포기하게 된다..

![8-1.png](/assets/img/8-1.png)

< ArrayBuffer 에 있는 바이너리 데이터 >

```js
import { FFT, fft_js } from "fft-js";

// 오디오 파일 로드 함수

async function loadAudio(url) {
  const audioContext = new AudioContext();
  const response = await fetch(url);
  const arrayBuffer = await response.arrayBuffer();
  const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
  return audioBuffer;
}

// 오디오 파일에서 MFCC 계산 함수
function calculateMFCC(audioBuffer) {
  const fft = new fft_js(buffer.length, options.sampleRate);
  let buffer = tf.buffer([numFrames, numMfcc]);
  const frameSize = 2048;
  const numCoefficients = 13;
  const melFilterBank = createMelFilterBank(
    numCoefficients,
    audioContext.sampleRate,
    0,
    audioContext.sampleRate / 2,
    frameSize
  );

  const audioData = audioBuffer.getChannelData(0);
  const audioDataLength = audioData.length;
  const MFCCs = [];

  for (let i = 0; i < audioDataLength; i += frameSize) {
    const audioDataChunk = audioData.slice(i, i + frameSize);

    // Hann Window 적용
    const windowedAudioDataChunk = audioDataChunk.map((value, index) => {
      const windowCoefficient =
        0.5 * (1 - Math.cos((2 * Math.PI * index) / (frameSize - 1)));
      return value * windowCoefficient;
    });

    // FFT 계산
    const frequencyDomainData = new Float32Array(frameSize);
    const fft = new fft(frameSize, audioContext.sampleRate);
    fft.forward(windowedAudioDataChunk, frequencyDomainData);

    // Mel 스케일로 변환
    const melDomainData = new Float32Array(numCoefficients);
    for (let j = 0; j < numCoefficients; j++) {
      let magnitude = 0;
      for (let k = 0; k < melFilterBank[j].length; k++) {
        magnitude += melFilterBank[j][k] * frequencyDomainData[k];
      }
      melDomainData[j] = Math.log10(magnitude);
    }

    // DCT 계산
    const MFCC = new Float32Array(numCoefficients);
    for (let j = 0; j < numCoefficients; j++) {
      let sum = 0;
      for (let k = 0; k < numCoefficients; k++) {
        sum +=
          melDomainData[k] *
          Math.cos(((Math.PI * j) / numCoefficients) * (k + 0.5));
      }
      MFCC[j] = sum;
    }

    MFCCs.push(MFCC);
  }

  return MFCCs;
}

// Mel 필터 계수 생성 함수
function createMelFilterBank(
  numFilters,
  sampleRate,
  minFreq,
  maxFreq,
  fftSize
) {
  const melFilterBank = [];

  // Mel 빈의 경계를 계산합니다.
  const melLowFreq = hertzToMel(minFreq);
  const melHighFreq = hertzToMel(maxFreq);
  const melBinWidth = (melHighFreq - melLowFreq) / (numFilters + 1);

  // 각 Mel 빈을 위한 삼각형 창을 생성합니다.
  for (let i = 0; i < numFilters; i++) {
    const currentMelCenterFreq = melLowFreq + melBinWidth * (i + 1);
    const currentCenterFreq = melToHertz(currentMelCenterFreq);
    const freqLeft =
      currentCenterFreq - melToHertz(currentMelCenterFreq - melBinWidth);
    const freqRight =
      currentCenterFreq + melToHertz(currentMelCenterFreq + melBinWidth);
    const filterBank = new Float32Array(fftSize);

    // 각 Mel 빈의 삼각형 창 계수를 계산합니다.
    for (let j = 0; j < fftSize; j++) {
      const freq = j * (sampleRate / fftSize);
      if (freq > freqLeft && freq < freqRight) {
        if (freq < currentCenterFreq) {
          filterBank[j] = (freq - freqLeft) / (currentCenterFreq - freqLeft);
        } else {
          filterBank[j] = (freqRight - freq) / (freqRight - currentCenterFreq);
        }
      }
    }

    melFilterBank.push(filterBank);
  }

  return melFilterBank;
}

// Hertz를 Mel로 변환하는 함수
function hertzToMel(frequency) {
  return 1125 * Math.log(1 + frequency / 700);
}

// Mel을 Hertz로 변환하는 함수
function melToHertz(mel) {
  return 700 * (Math.exp(mel / 1125) - 1);
}

// 두 오디오의 MFCC를 비교하여 유사도를 계산하는 함수
function compareMFCCs(mfcc1, mfcc2) {
  // 두 배열의 길이가 다르면 에러 반환
  if (mfcc1.length !== mfcc2.length) {
    throw new Error("MFCC lengths do not match");
  }

  const numFrames = mfcc1.length;
  let distance = 0;

  for (let i = 0; i < numFrames; i++) {
    const frame1 = mfcc1[i];
    const frame2 = mfcc2[i];

    // 두 프레임 간의 코사인 거리 계산
    let dotProduct = 0;
    let magnitude1 = 0;
    let magnitude2 = 0;
    for (let j = 0; j < frame1.length; j++) {
      dotProduct += frame1[j] * frame2[j];
      magnitude1 += frame1[j] ** 2;
      magnitude2 += frame2[j] ** 2;
    }
    const similarity =
      dotProduct / (Math.sqrt(magnitude1) * Math.sqrt(magnitude2));

    // 거리에 추가
    distance += 1 - similarity;
  }

  // 두 MFCC 배열 간의 거리 반환
  return distance / numFrames;
}

// 오디오 파일 분석 함수
async function analyzeAudio(url1, url2) {
  try {
    // 오디오 파일 로드
    const audioBuffer1 = await loadAudio(url1);
    const audioBuffer2 = await loadAudio(url2);

    // MFCC 계산
    const MFCCs1 = calculateMFCC(audioBuffer1);
    const MFCCs2 = calculateMFCC(audioBuffer2);

    // 두 MFCC 배열 간의 거리 계산
    const distance = compareMFCCs(MFCCs1, MFCCs2);

    // 유사도 출력
    const similarity = (1 - distance) * 100;
    console.log(`두 노래의 유사도는 ${similarity}% 입니다.`);
  } catch (error) {
    console.error(error);
  }
  // console.log(fft_js);
}

export { analyzeAudio };
```

<br>

Tensorflow.js 나 파이썬을 통한 librosa 라이브러리도 도전하다 번번이 실패하게 된다. 결국 SingIt 개발 중단을 때리고 차후 hum to search 오픈소스가 세상으로 나오게 되면 마저 개발해 볼까 한다.

![8.png](/assets/img/8.png)
